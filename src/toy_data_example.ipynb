{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-30 20:20:46,732 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put ../data/* /user/bosche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-30 20:20:49,740 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   1 ryanbusby supergroup        302 2018-11-30 20:20 /user/bosche/toyTest.csv\n",
      "-rw-r--r--   1 ryanbusby supergroup        343 2018-11-30 20:20 /user/bosche/toyTrain.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/bosche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "from tqdm import tqdm\n",
    "from pyspark.sql.functions import when\n",
    "from datetime import datetime\n",
    "\n",
    "def myMungeNoLabel(path, spark):\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "\n",
    "    print('counting observations'.upper())\n",
    "    num_obs = df.rdd.map(lambda x:(x[0], counts(x, label=False)))\\\n",
    "    .toDF(['ii','Counts'])\n",
    "\n",
    "    print('labeling outliers'.upper())\n",
    "    df = multi_nameOuts(df, 1.5, label=False)(df)\n",
    "\n",
    "    print('summing outliers'.upper())\n",
    "    df = df.rdd.map(lambda x:(x[0],sum(x[1:]))).toDF(['Id','Outliers'])\n",
    "\n",
    "    print('joining'.upper())\n",
    "    df = df.join(num_obs, df.Id == num_obs.ii, 'left')\\\n",
    "    .select('Id','Outliers', 'Counts')\n",
    "\n",
    "    cols = ['Id','Outliers', 'Obs', 'Outs']\n",
    "    df = df.rdd.map(lambda x:(x[0],x[1],x[2][0],x[2][1])).toDF(cols)\n",
    "\n",
    "    return df\n",
    "\n",
    "def myMunge(path, spark):\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "\n",
    "    print('counting observations'.upper())\n",
    "    num_obs = df.rdd.map(lambda x:(x[0], counts(x), x[-1]))\\\n",
    "    .toDF(['ii','Counts', 'Response'])\n",
    "\n",
    "    print('labeling outliers'.upper())\n",
    "    df = multi_nameOuts(df,1.5)(df)\n",
    "\n",
    "    print('summing outliers'.upper())\n",
    "    df = df.rdd.map(lambda x:(x[0],sum(x[1:-1]))).toDF(['Id','Outliers'])\n",
    "\n",
    "    print('joining'.upper())\n",
    "    df = df.join(num_obs, df.Id == num_obs.ii, 'left')\\\n",
    "    .select('Outliers', 'Counts', 'Response')\n",
    "\n",
    "    cols = ['Outliers', 'Obs', 'Outs', 'Response']\n",
    "    df = df.rdd.map(lambda x:(x[0],x[1][0],x[1][1],x[2])).toDF(cols)\n",
    "\n",
    "    print('balancing classes'.upper())\n",
    "    df = balance_classes(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def counts(x, label=True):\n",
    "    obs, outs = 0, 0\n",
    "    end = -1\n",
    "    if not label:\n",
    "        end = None\n",
    "    for xx in x[1:end]:\n",
    "        if xx:\n",
    "            obs += 1\n",
    "            if xx > .25 or xx < -.25:\n",
    "                outs += 1\n",
    "    return obs, outs\n",
    "\n",
    "def nameOuts(df, col_name, iqrx):\n",
    "    quants = df.approxQuantile([col_name],[.25,.75],.5)\n",
    "    q1, q3 = quants[0][0], quants[0][1]\n",
    "    iqr = q3 - q1\n",
    "    lb = q1 - iqrx * iqr\n",
    "    ub = q3 + iqrx * iqr\n",
    "    return when((df[col_name]<lb) | (df[col_name]>ub),1).otherwise(0)\n",
    "\n",
    "def multi_nameOuts(df, iqrx, label=True):\n",
    "    # USE approxQuantile() TO CALCULATE THE IQR PER COLUMN AND LABEL OUTS\n",
    "    end = -1\n",
    "    if not label:\n",
    "        end = None\n",
    "    def inner(dataframe):\n",
    "        for col_name in tqdm(df.columns[1:end]):\n",
    "            dataframe = dataframe.withColumn(col_name,\\\n",
    "                               nameOuts(df, col_name, iqrx))\n",
    "        return dataframe\n",
    "    return inner\n",
    "\n",
    "def balance_classes(df):\n",
    "    # OVERSAMPLING SPECIFICALLY TO ADDRESS CLASS IMBALANCE OF BOSCHE DATA\n",
    "    '''\n",
    "    fraction argument in .sample() misbehaves\n",
    "    if it didn't should be able to return without while loop\n",
    "    '''\n",
    "    c0 = df.filter(df.Response==0).count()\n",
    "    c1 = df.filter(df.Response==1).count()\n",
    "    diff = float(abs(c0 - c1))\n",
    "    lrgrClss = max(c0, c1)\n",
    "    smlrClss = min(c0, c1)\n",
    "    if smlrClss == 0:\n",
    "        smlrClss = 1\n",
    "    x = diff / smlrClss\n",
    "    f_label = 0\n",
    "    if c0 > c1:\n",
    "        f_label = 1\n",
    "    if x < .25:\n",
    "        return df\n",
    "    else:\n",
    "        while smlrClss+df.filter(df.Response==f_label)\\\n",
    "        .sample(True, x, 42).count() < .9*lrgrClss:\n",
    "            x += x/2\n",
    "    return df.union(df.filter(df.Response==f_label).sample(True,x,42))\n",
    "\n",
    "def save_munged(X, file_name):\n",
    "    dt = datetime.now().time()\n",
    "    munged_file_name = str(dt).replace(':', '_') + '_' + file_name\n",
    "    munged_path = root % munged_file_name\n",
    "    print('saving data >>> '.upper() + munged_path)\n",
    "    X.write.csv(munged_path, header=True)\n",
    "    return munged_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkContext = ps.SparkContext(master='spark://ryans-macbook:7077')\n",
    "spark = ps.sql.SparkSession(sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNTING OBSERVATIONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABELING OUTLIERS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMING OUTLIERS\n",
      "JOINING\n",
      "BALANCING CLASSES\n",
      "SAVING DATA >>> hdfs://ryans-macbook:9000/user/bosche/20_21_10.434825_toyTrain.csv\n"
     ]
    }
   ],
   "source": [
    "root = 'hdfs://ryans-macbook:9000/user/bosche/%s'\n",
    "train_file_name = 'toyTrain.csv'\n",
    "train_path = root % train_file_name\n",
    "X = myMunge(train_path, spark)\n",
    "munged_train_path = save_munged(X, train_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----+--------+\n",
      "|Outliers|Obs|Outs|Response|\n",
      "+--------+---+----+--------+\n",
      "|       0|  3|   0|       0|\n",
      "|       0|  3|   0|       0|\n",
      "|       0|  1|   0|       1|\n",
      "|       0|  2|   0|       0|\n",
      "|       0|  2|   0|       0|\n",
      "|       0|  1|   0|       0|\n",
      "|       0|  2|   0|       0|\n",
      "|       0|  1|   1|       0|\n",
      "|       0|  1|   0|       0|\n",
      "|       0|  2|   0|       1|\n",
      "|       0|  2|   0|       0|\n",
      "|       0|  3|   1|       0|\n",
      "|       0|  2|   1|       0|\n",
      "|       0|  1|   0|       1|\n",
      "|       0|  1|   0|       0|\n",
      "|       0|  1|   1|       0|\n",
      "|       0|  1|   0|       1|\n",
      "|       0|  3|   1|       0|\n",
      "|       0|  1|   0|       1|\n",
      "|       0|  2|   0|       1|\n",
      "+--------+---+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNTING OBSERVATIONS\n",
      "LABELING OUTLIERS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  9.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMING OUTLIERS\n",
      "JOINING\n",
      "SAVING DATA >>> hdfs://ryans-macbook:9000/user/bosche/20_21_16.546415_toyTest.csv\n"
     ]
    }
   ],
   "source": [
    "test_file_name = 'toyTest.csv'\n",
    "test_path = root % test_file_name\n",
    "X = myMungeNoLabel(test_path, spark)\n",
    "munged_test_path = save_munged(X, test_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+----+\n",
      "| Id|Outliers|Obs|Outs|\n",
      "+---+--------+---+----+\n",
      "|  0|       0|  3|   0|\n",
      "|  7|       0|  3|   0|\n",
      "|  6|       0|  1|   0|\n",
      "|  9|       0|  2|   0|\n",
      "| 17|       0|  2|   0|\n",
      "|  5|       0|  1|   0|\n",
      "|  1|       0|  2|   0|\n",
      "| 10|       0|  1|   1|\n",
      "|  3|       0|  1|   0|\n",
      "| 12|       0|  2|   0|\n",
      "|  8|       0|  2|   0|\n",
      "| 11|       0|  3|   1|\n",
      "|  2|       0|  2|   1|\n",
      "|  4|       0|  1|   0|\n",
      "| 13|       0|  1|   0|\n",
      "| 14|       0|  1|   1|\n",
      "| 15|       0|  1|   0|\n",
      "| 16|       0|  3|   1|\n",
      "+---+--------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X.show(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "def run(spark, root, train_path, test_path):\n",
    "    model, X_test = trainModel(spark, train_path)\n",
    "    validate_save_model(model, X_test)\n",
    "    make_save_preds(model, test_path)\n",
    "\n",
    "def load_data(path, persisted=True, test=False):\n",
    "    if not persisted:\n",
    "        if test:\n",
    "            df = myMunge(path, spark, labeled=False)\n",
    "            return df\n",
    "        df = myMunge(path, spark)\n",
    "        return df\n",
    "    else:\n",
    "        df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "        return df\n",
    "\n",
    "def vectorize(df, test=False):\n",
    "    numericCols = ['Obs', 'Outs', 'Outliers']\n",
    "    assembler = VectorAssembler(inputCols=numericCols,\\\n",
    "                                outputCol='features')\n",
    "\n",
    "    stages = [assembler]\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    pipelineModel = pipeline.fit(df)\n",
    "    cols = ['features', 'Response']\n",
    "    if test:\n",
    "        cols = ['Id','features']\n",
    "    df = pipelineModel.transform(df)\n",
    "    return df\n",
    "\n",
    "def trainModel(spark, train_path):\n",
    "    train_df = load_data(train_path)\n",
    "    train_df = vectorize(train_df)\n",
    "\n",
    "    X_train, X_test = train_df.randomSplit([.8, .2], 42)\n",
    "\n",
    "    # utilize pyspark.ml.tuning here to gridsearch and tune the model\n",
    "    lr = LogisticRegression(featuresCol='features',\\\n",
    "                            labelCol='Response',\\\n",
    "                            maxIter=2,\\\n",
    "                            regParam=.3,\\\n",
    "                            elasticNetParam=.8)\n",
    "\n",
    "    lrModel = lr.fit(X_train)\n",
    "    return lrModel, X_test\n",
    "\n",
    "def validate_save_model(model, X_test):\n",
    "    bce = BinaryClassificationEvaluator(labelCol='Response')\n",
    "    train_preds = model.transform(X_test)\n",
    "    score = bce.evaluate(train_preds)\n",
    "    print('The Model got a %s of %s' % (bce.getMetricName(), score))\n",
    "    dt = datetime.now().time()\n",
    "    date_name = str(dt).replace(':', '_')\n",
    "    model.save('../models/%s_LR' % date_name)\n",
    "\n",
    "def make_save_preds(model, test_path):\n",
    "    test_df = load_data(test_path)\n",
    "    test_df = vectorize(test_df, test=True)\n",
    "    preds = model.transform(test_df).select('Id', 'prediction')\n",
    "    dt = datetime.now().time()\n",
    "    date_name = str(dt).replace(':', '_')\n",
    "    preds.write.csv('%s' % root % date_name + '_PREDS.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Model got a areaUnderROC of 0.5\n"
     ]
    }
   ],
   "source": [
    "run(spark, root, munged_train_path, munged_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-30 20:21:42,819 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 5 items\n",
      "drwxr-xr-x   - ryanbusby supergroup          0 2018-11-30 20:21 /user/bosche/20_21_10.434825_toyTrain.csv\n",
      "drwxr-xr-x   - ryanbusby supergroup          0 2018-11-30 20:21 /user/bosche/20_21_16.546415_toyTest.csv\n",
      "drwxr-xr-x   - ryanbusby supergroup          0 2018-11-30 20:21 /user/bosche/20_21_28.625402_PREDS.csv\n",
      "-rw-r--r--   1 ryanbusby supergroup        302 2018-11-30 20:20 /user/bosche/toyTest.csv\n",
      "-rw-r--r--   1 ryanbusby supergroup        343 2018-11-30 20:20 /user/bosche/toyTrain.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/bosche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
